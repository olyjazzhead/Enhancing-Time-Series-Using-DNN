{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "543ad74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a08cbc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the datasets (If doing Train Test Split in this script)\n",
    "\n",
    "df = pd.read_csv('TrainingSets/TrainingData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a000992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Vol.</th>\n",
       "      <th>Change %</th>\n",
       "      <th>return_1d</th>\n",
       "      <th>return_21d</th>\n",
       "      <th>return_63d</th>\n",
       "      <th>macd</th>\n",
       "      <th>macd_signal</th>\n",
       "      <th>macd_hist</th>\n",
       "      <th>volatility</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02/14/1995</td>\n",
       "      <td>-0.503264</td>\n",
       "      <td>-0.495490</td>\n",
       "      <td>-0.507674</td>\n",
       "      <td>-0.483120</td>\n",
       "      <td>-0.449743</td>\n",
       "      <td>-0.073927</td>\n",
       "      <td>-0.076736</td>\n",
       "      <td>-0.064437</td>\n",
       "      <td>-0.113838</td>\n",
       "      <td>0.007077</td>\n",
       "      <td>0.001755</td>\n",
       "      <td>0.017732</td>\n",
       "      <td>-0.730385</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02/15/1995</td>\n",
       "      <td>-0.529106</td>\n",
       "      <td>-0.500564</td>\n",
       "      <td>-0.522569</td>\n",
       "      <td>-0.506429</td>\n",
       "      <td>-0.324925</td>\n",
       "      <td>-0.615747</td>\n",
       "      <td>-0.627913</td>\n",
       "      <td>-0.064437</td>\n",
       "      <td>-0.113838</td>\n",
       "      <td>-0.032836</td>\n",
       "      <td>-0.005585</td>\n",
       "      <td>-0.090127</td>\n",
       "      <td>-0.657557</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02/16/1995</td>\n",
       "      <td>-0.526522</td>\n",
       "      <td>-0.522128</td>\n",
       "      <td>-0.538705</td>\n",
       "      <td>-0.514199</td>\n",
       "      <td>-0.406680</td>\n",
       "      <td>0.045984</td>\n",
       "      <td>0.046483</td>\n",
       "      <td>-0.064437</td>\n",
       "      <td>-0.113838</td>\n",
       "      <td>-0.060570</td>\n",
       "      <td>-0.017360</td>\n",
       "      <td>-0.144584</td>\n",
       "      <td>-0.808153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02/17/1995</td>\n",
       "      <td>-0.526522</td>\n",
       "      <td>-0.519591</td>\n",
       "      <td>-0.537464</td>\n",
       "      <td>-0.506429</td>\n",
       "      <td>-0.447994</td>\n",
       "      <td>-0.016192</td>\n",
       "      <td>-0.015586</td>\n",
       "      <td>-0.064437</td>\n",
       "      <td>-0.113838</td>\n",
       "      <td>-0.081905</td>\n",
       "      <td>-0.031322</td>\n",
       "      <td>-0.171426</td>\n",
       "      <td>-0.853840</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02/21/1995</td>\n",
       "      <td>-0.558824</td>\n",
       "      <td>-0.524665</td>\n",
       "      <td>-0.547393</td>\n",
       "      <td>-0.532329</td>\n",
       "      <td>-0.374328</td>\n",
       "      <td>-0.771186</td>\n",
       "      <td>-0.790386</td>\n",
       "      <td>-0.064437</td>\n",
       "      <td>-0.113838</td>\n",
       "      <td>-0.141218</td>\n",
       "      <td>-0.055116</td>\n",
       "      <td>-0.292156</td>\n",
       "      <td>-0.756758</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4979</th>\n",
       "      <td>02/12/2014</td>\n",
       "      <td>0.606036</td>\n",
       "      <td>0.608099</td>\n",
       "      <td>0.604467</td>\n",
       "      <td>0.637717</td>\n",
       "      <td>0.284517</td>\n",
       "      <td>-0.260455</td>\n",
       "      <td>-0.266400</td>\n",
       "      <td>0.195159</td>\n",
       "      <td>-0.746172</td>\n",
       "      <td>-0.241849</td>\n",
       "      <td>-0.705371</td>\n",
       "      <td>1.375110</td>\n",
       "      <td>-0.522140</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4980</th>\n",
       "      <td>02/13/2014</td>\n",
       "      <td>0.650587</td>\n",
       "      <td>0.593536</td>\n",
       "      <td>0.597665</td>\n",
       "      <td>0.635127</td>\n",
       "      <td>0.284517</td>\n",
       "      <td>0.636656</td>\n",
       "      <td>0.653340</td>\n",
       "      <td>0.234105</td>\n",
       "      <td>-0.650730</td>\n",
       "      <td>-0.113907</td>\n",
       "      <td>-0.588542</td>\n",
       "      <td>1.434463</td>\n",
       "      <td>-0.499596</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4981</th>\n",
       "      <td>02/14/2014</td>\n",
       "      <td>0.666093</td>\n",
       "      <td>0.636665</td>\n",
       "      <td>0.628696</td>\n",
       "      <td>0.682368</td>\n",
       "      <td>0.284517</td>\n",
       "      <td>0.205865</td>\n",
       "      <td>0.213843</td>\n",
       "      <td>0.542856</td>\n",
       "      <td>-0.607582</td>\n",
       "      <td>0.008087</td>\n",
       "      <td>-0.469112</td>\n",
       "      <td>1.466400</td>\n",
       "      <td>-0.507162</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4982</th>\n",
       "      <td>02/18/2014</td>\n",
       "      <td>0.741707</td>\n",
       "      <td>0.658838</td>\n",
       "      <td>0.679586</td>\n",
       "      <td>0.705056</td>\n",
       "      <td>0.284517</td>\n",
       "      <td>1.071888</td>\n",
       "      <td>1.097647</td>\n",
       "      <td>0.735365</td>\n",
       "      <td>-0.437462</td>\n",
       "      <td>0.204482</td>\n",
       "      <td>-0.331765</td>\n",
       "      <td>1.686395</td>\n",
       "      <td>-0.432448</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4983</th>\n",
       "      <td>02/19/2014</td>\n",
       "      <td>0.784346</td>\n",
       "      <td>0.722922</td>\n",
       "      <td>0.715582</td>\n",
       "      <td>0.762036</td>\n",
       "      <td>0.284517</td>\n",
       "      <td>0.583362</td>\n",
       "      <td>0.597173</td>\n",
       "      <td>1.104172</td>\n",
       "      <td>-0.427007</td>\n",
       "      <td>0.412795</td>\n",
       "      <td>-0.177547</td>\n",
       "      <td>1.893541</td>\n",
       "      <td>-0.801462</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4984 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date     Close      Open      High       Low      Vol.  Change %  \\\n",
       "0     02/14/1995 -0.503264 -0.495490 -0.507674 -0.483120 -0.449743 -0.073927   \n",
       "1     02/15/1995 -0.529106 -0.500564 -0.522569 -0.506429 -0.324925 -0.615747   \n",
       "2     02/16/1995 -0.526522 -0.522128 -0.538705 -0.514199 -0.406680  0.045984   \n",
       "3     02/17/1995 -0.526522 -0.519591 -0.537464 -0.506429 -0.447994 -0.016192   \n",
       "4     02/21/1995 -0.558824 -0.524665 -0.547393 -0.532329 -0.374328 -0.771186   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "4979  02/12/2014  0.606036  0.608099  0.604467  0.637717  0.284517 -0.260455   \n",
       "4980  02/13/2014  0.650587  0.593536  0.597665  0.635127  0.284517  0.636656   \n",
       "4981  02/14/2014  0.666093  0.636665  0.628696  0.682368  0.284517  0.205865   \n",
       "4982  02/18/2014  0.741707  0.658838  0.679586  0.705056  0.284517  1.071888   \n",
       "4983  02/19/2014  0.784346  0.722922  0.715582  0.762036  0.284517  0.583362   \n",
       "\n",
       "      return_1d  return_21d  return_63d      macd  macd_signal  macd_hist  \\\n",
       "0     -0.076736   -0.064437   -0.113838  0.007077     0.001755   0.017732   \n",
       "1     -0.627913   -0.064437   -0.113838 -0.032836    -0.005585  -0.090127   \n",
       "2      0.046483   -0.064437   -0.113838 -0.060570    -0.017360  -0.144584   \n",
       "3     -0.015586   -0.064437   -0.113838 -0.081905    -0.031322  -0.171426   \n",
       "4     -0.790386   -0.064437   -0.113838 -0.141218    -0.055116  -0.292156   \n",
       "...         ...         ...         ...       ...          ...        ...   \n",
       "4979  -0.266400    0.195159   -0.746172 -0.241849    -0.705371   1.375110   \n",
       "4980   0.653340    0.234105   -0.650730 -0.113907    -0.588542   1.434463   \n",
       "4981   0.213843    0.542856   -0.607582  0.008087    -0.469112   1.466400   \n",
       "4982   1.097647    0.735365   -0.437462  0.204482    -0.331765   1.686395   \n",
       "4983   0.597173    1.104172   -0.427007  0.412795    -0.177547   1.893541   \n",
       "\n",
       "      volatility  Target  \n",
       "0      -0.730385       0  \n",
       "1      -0.657557       1  \n",
       "2      -0.808153       0  \n",
       "3      -0.853840       0  \n",
       "4      -0.756758       1  \n",
       "...          ...     ...  \n",
       "4979   -0.522140       1  \n",
       "4980   -0.499596       1  \n",
       "4981   -0.507162       1  \n",
       "4982   -0.432448       1  \n",
       "4983   -0.801462       0  \n",
       "\n",
       "[4984 rows x 15 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61038275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use only if need Date in training data. Since we are doing classification we can drop the date\n",
    "Date = False\n",
    "if Date == True:\n",
    "    if 'Date' in df.columns:\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df['Month'] = df['Date'].dt.month\n",
    "        df['Day'] = df['Date'].dt.day\n",
    "        df['Year'] = df['Date'].dt.day\n",
    "        df['Weekday'] = df['Date'].dt.weekday\n",
    "        df.drop('Date', axis=1, inplace=True)\n",
    "else:\n",
    "    df.drop('Date', axis=1, inplace=True)  # Optional: remove if not needed for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "55bdd1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Close', 'Open', 'High', 'Low', 'Vol.', 'Change %', 'return_1d',\n",
       "       'return_21d', 'return_63d', 'macd', 'macd_signal', 'macd_hist',\n",
       "       'volatility', 'Target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check columns of df\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5a2d0a",
   "metadata": {},
   "source": [
    "## Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9519a8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delcare Target\n",
    "#WARNING : there should be no selection nor shuffling later on ! (otherwise misalignement)\n",
    "target = df[\"Target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c0b3eed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Df shape of dataset to be used : (4984, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Vol.</th>\n",
       "      <th>Change %</th>\n",
       "      <th>return_1d</th>\n",
       "      <th>macd</th>\n",
       "      <th>macd_signal</th>\n",
       "      <th>macd_hist</th>\n",
       "      <th>volatility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.503264</td>\n",
       "      <td>-0.495490</td>\n",
       "      <td>-0.507674</td>\n",
       "      <td>-0.483120</td>\n",
       "      <td>-0.449743</td>\n",
       "      <td>-0.073927</td>\n",
       "      <td>-0.076736</td>\n",
       "      <td>0.007077</td>\n",
       "      <td>0.001755</td>\n",
       "      <td>0.017732</td>\n",
       "      <td>-0.730385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.529106</td>\n",
       "      <td>-0.500564</td>\n",
       "      <td>-0.522569</td>\n",
       "      <td>-0.506429</td>\n",
       "      <td>-0.324925</td>\n",
       "      <td>-0.615747</td>\n",
       "      <td>-0.627913</td>\n",
       "      <td>-0.032836</td>\n",
       "      <td>-0.005585</td>\n",
       "      <td>-0.090127</td>\n",
       "      <td>-0.657557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.526522</td>\n",
       "      <td>-0.522128</td>\n",
       "      <td>-0.538705</td>\n",
       "      <td>-0.514199</td>\n",
       "      <td>-0.406680</td>\n",
       "      <td>0.045984</td>\n",
       "      <td>0.046483</td>\n",
       "      <td>-0.060570</td>\n",
       "      <td>-0.017360</td>\n",
       "      <td>-0.144584</td>\n",
       "      <td>-0.808153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.526522</td>\n",
       "      <td>-0.519591</td>\n",
       "      <td>-0.537464</td>\n",
       "      <td>-0.506429</td>\n",
       "      <td>-0.447994</td>\n",
       "      <td>-0.016192</td>\n",
       "      <td>-0.015586</td>\n",
       "      <td>-0.081905</td>\n",
       "      <td>-0.031322</td>\n",
       "      <td>-0.171426</td>\n",
       "      <td>-0.853840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.558824</td>\n",
       "      <td>-0.524665</td>\n",
       "      <td>-0.547393</td>\n",
       "      <td>-0.532329</td>\n",
       "      <td>-0.374328</td>\n",
       "      <td>-0.771186</td>\n",
       "      <td>-0.790386</td>\n",
       "      <td>-0.141218</td>\n",
       "      <td>-0.055116</td>\n",
       "      <td>-0.292156</td>\n",
       "      <td>-0.756758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Close      Open      High       Low      Vol.  Change %  return_1d  \\\n",
       "0 -0.503264 -0.495490 -0.507674 -0.483120 -0.449743 -0.073927  -0.076736   \n",
       "1 -0.529106 -0.500564 -0.522569 -0.506429 -0.324925 -0.615747  -0.627913   \n",
       "2 -0.526522 -0.522128 -0.538705 -0.514199 -0.406680  0.045984   0.046483   \n",
       "3 -0.526522 -0.519591 -0.537464 -0.506429 -0.447994 -0.016192  -0.015586   \n",
       "4 -0.558824 -0.524665 -0.547393 -0.532329 -0.374328 -0.771186  -0.790386   \n",
       "\n",
       "       macd  macd_signal  macd_hist  volatility  \n",
       "0  0.007077     0.001755   0.017732   -0.730385  \n",
       "1 -0.032836    -0.005585  -0.090127   -0.657557  \n",
       "2 -0.060570    -0.017360  -0.144584   -0.808153  \n",
       "3 -0.081905    -0.031322  -0.171426   -0.853840  \n",
       "4 -0.141218    -0.055116  -0.292156   -0.756758  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    0\n",
       "3    0\n",
       "4    1\n",
       "Name: Target, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Drop Unnecessary columns\n",
    "# For simplicity only keep some most important features. Can add more later down below\n",
    "data=pd.DataFrame(df, columns=['Close', 'Open', 'High', 'Low', 'Vol.', 'Change %', 'return_1d','macd', 'macd_signal', 'macd_hist',\n",
    "       'volatility'])\n",
    "\n",
    "print (\"Df shape of dataset to be used :\",data.shape)\n",
    "display(data.head())\n",
    "display(target.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613bd0bc",
   "metadata": {},
   "source": [
    "## PreProcess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d61a13a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training sets (If necessary)\n",
    "X_train = pd.read_csv('TrainingSets/X_train_scaled.csv')#, index_col=0)\n",
    "y_train = pd.read_csv('TrainingSets/y_train.csv')#, index_col=0)\n",
    "X_test = pd.read_csv('TrainingSets/X_test_scaled.csv')#, index_col=0)\n",
    "y_test = pd.read_csv('TrainingSets/y_test.csv')#, index_col=0)\n",
    "#X_val = pd.read_csv('X_val_scaled.csv', index_col=0)\n",
    "#y_val = pd.read_csv('y_val.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "daae4d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "random_state=42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "19394abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional....split into a train, test and validation set\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(data, target, test_size=0.20, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e84bdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (3987, 11)\n",
      "y_train shape: (3987,)\n",
      "X_test shape: (997, 11)\n",
      "y_test shape: (997,)\n"
     ]
    }
   ],
   "source": [
    "#Reset index for dataseries, not needed for ndarray (X_train, X_test)\n",
    "#Basically just re-adding the original element indexing from pandas\n",
    "y_train, y_test = \\\n",
    "    y_train.reset_index(drop=True),y_test.reset_index(drop=True)\n",
    "\n",
    "print (\"X_train shape:\",X_train.shape)\n",
    "print (\"y_train shape:\",y_train.shape)\n",
    "\n",
    "\n",
    "print (\"X_test shape:\",X_test.shape)\n",
    "print (\"y_test shape:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b1b7bd",
   "metadata": {},
   "source": [
    "## Build the Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a036c94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "94/94 [==============================] - 1s 5ms/step - loss: -0.1054 - mean_squared_error: 2.1463 - val_loss: -1.0488 - val_mean_squared_error: 1.4712\n",
      "Epoch 2/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -1.6502 - mean_squared_error: 2.4472 - val_loss: -2.4981 - val_mean_squared_error: 3.5019\n",
      "Epoch 3/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -2.2651 - mean_squared_error: 4.5746 - val_loss: -2.9240 - val_mean_squared_error: 5.2353\n",
      "Epoch 4/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -2.6613 - mean_squared_error: 5.8918 - val_loss: -3.1467 - val_mean_squared_error: 6.4941\n",
      "Epoch 5/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -2.7284 - mean_squared_error: 7.4179 - val_loss: -3.3051 - val_mean_squared_error: 7.6234\n",
      "Epoch 6/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -2.9512 - mean_squared_error: 8.4239 - val_loss: -3.4506 - val_mean_squared_error: 8.6511\n",
      "Epoch 7/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -3.1399 - mean_squared_error: 9.4092 - val_loss: -3.6056 - val_mean_squared_error: 9.3570\n",
      "Epoch 8/50\n",
      "94/94 [==============================] - 0s 4ms/step - loss: -3.2626 - mean_squared_error: 9.9531 - val_loss: -3.7616 - val_mean_squared_error: 9.6500\n",
      "Epoch 9/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -3.3562 - mean_squared_error: 10.3086 - val_loss: -3.8883 - val_mean_squared_error: 10.4350\n",
      "Epoch 10/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -3.4774 - mean_squared_error: 11.0305 - val_loss: -4.0354 - val_mean_squared_error: 10.8632\n",
      "Epoch 11/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -3.6024 - mean_squared_error: 11.5109 - val_loss: -4.1813 - val_mean_squared_error: 11.1124\n",
      "Epoch 12/50\n",
      "94/94 [==============================] - 0s 4ms/step - loss: -3.6633 - mean_squared_error: 11.4378 - val_loss: -4.3511 - val_mean_squared_error: 11.0137\n",
      "Epoch 13/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -3.7635 - mean_squared_error: 11.6598 - val_loss: -4.4804 - val_mean_squared_error: 11.3099\n",
      "Epoch 14/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -3.9267 - mean_squared_error: 11.8863 - val_loss: -4.6546 - val_mean_squared_error: 11.4967\n",
      "Epoch 15/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -4.1172 - mean_squared_error: 12.0582 - val_loss: -4.8261 - val_mean_squared_error: 11.5684\n",
      "Epoch 16/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -4.2117 - mean_squared_error: 12.0001 - val_loss: -4.9944 - val_mean_squared_error: 11.6930\n",
      "Epoch 17/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -4.3308 - mean_squared_error: 12.1295 - val_loss: -5.2363 - val_mean_squared_error: 11.6523\n",
      "Epoch 18/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -4.4782 - mean_squared_error: 11.9573 - val_loss: -5.4492 - val_mean_squared_error: 11.4426\n",
      "Epoch 19/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -4.5528 - mean_squared_error: 11.6474 - val_loss: -5.6714 - val_mean_squared_error: 11.1290\n",
      "Epoch 20/50\n",
      "94/94 [==============================] - 0s 4ms/step - loss: -4.7066 - mean_squared_error: 11.8049 - val_loss: -5.8891 - val_mean_squared_error: 11.1282\n",
      "Epoch 21/50\n",
      "94/94 [==============================] - 0s 5ms/step - loss: -4.8864 - mean_squared_error: 11.6519 - val_loss: -6.1858 - val_mean_squared_error: 11.0314\n",
      "Epoch 22/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -5.0674 - mean_squared_error: 11.4798 - val_loss: -6.4706 - val_mean_squared_error: 10.7117\n",
      "Epoch 23/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -5.2806 - mean_squared_error: 11.0428 - val_loss: -6.7463 - val_mean_squared_error: 10.8029\n",
      "Epoch 24/50\n",
      "94/94 [==============================] - 0s 5ms/step - loss: -5.3431 - mean_squared_error: 11.1790 - val_loss: -7.0419 - val_mean_squared_error: 10.9390\n",
      "Epoch 25/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -5.5937 - mean_squared_error: 11.2166 - val_loss: -7.4283 - val_mean_squared_error: 10.6910\n",
      "Epoch 26/50\n",
      "94/94 [==============================] - 0s 4ms/step - loss: -5.7487 - mean_squared_error: 10.8450 - val_loss: -7.7586 - val_mean_squared_error: 10.3457\n",
      "Epoch 27/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -5.9221 - mean_squared_error: 10.8811 - val_loss: -8.1614 - val_mean_squared_error: 10.6217\n",
      "Epoch 28/50\n",
      "94/94 [==============================] - 0s 4ms/step - loss: -5.9382 - mean_squared_error: 10.9391 - val_loss: -8.5335 - val_mean_squared_error: 10.5399\n",
      "Epoch 29/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -6.0918 - mean_squared_error: 10.8226 - val_loss: -9.0083 - val_mean_squared_error: 10.1615\n",
      "Epoch 30/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -6.4009 - mean_squared_error: 10.5109 - val_loss: -9.6325 - val_mean_squared_error: 10.0488\n",
      "Epoch 31/50\n",
      "94/94 [==============================] - 0s 4ms/step - loss: -6.6659 - mean_squared_error: 10.4882 - val_loss: -9.9865 - val_mean_squared_error: 10.2527\n",
      "Epoch 32/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -6.8439 - mean_squared_error: 10.5932 - val_loss: -10.4504 - val_mean_squared_error: 10.3635\n",
      "Epoch 33/50\n",
      "94/94 [==============================] - 0s 4ms/step - loss: -6.9727 - mean_squared_error: 10.6962 - val_loss: -10.9223 - val_mean_squared_error: 10.5016\n",
      "Epoch 34/50\n",
      "94/94 [==============================] - 0s 4ms/step - loss: -7.2154 - mean_squared_error: 10.5977 - val_loss: -11.6653 - val_mean_squared_error: 10.3711\n",
      "Epoch 35/50\n",
      "94/94 [==============================] - 0s 4ms/step - loss: -7.4254 - mean_squared_error: 10.6344 - val_loss: -12.2403 - val_mean_squared_error: 10.4891\n",
      "Epoch 36/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -7.5200 - mean_squared_error: 10.7163 - val_loss: -12.9458 - val_mean_squared_error: 10.3492\n",
      "Epoch 37/50\n",
      "94/94 [==============================] - 0s 4ms/step - loss: -8.0229 - mean_squared_error: 10.8411 - val_loss: -13.6236 - val_mean_squared_error: 10.3651\n",
      "Epoch 38/50\n",
      "94/94 [==============================] - 0s 4ms/step - loss: -7.8684 - mean_squared_error: 10.9187 - val_loss: -14.1675 - val_mean_squared_error: 10.6589\n",
      "Epoch 39/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -7.9798 - mean_squared_error: 10.9472 - val_loss: -14.7588 - val_mean_squared_error: 10.8637\n",
      "Epoch 40/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -8.4193 - mean_squared_error: 11.0401 - val_loss: -15.6261 - val_mean_squared_error: 10.6042\n",
      "Epoch 41/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -8.5493 - mean_squared_error: 10.7774 - val_loss: -16.1382 - val_mean_squared_error: 10.8121\n",
      "Epoch 42/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -8.8681 - mean_squared_error: 11.0110 - val_loss: -17.2554 - val_mean_squared_error: 10.7565\n",
      "Epoch 43/50\n",
      "94/94 [==============================] - 0s 4ms/step - loss: -9.1554 - mean_squared_error: 11.0110 - val_loss: -17.6062 - val_mean_squared_error: 10.6234\n",
      "Epoch 44/50\n",
      "94/94 [==============================] - 0s 4ms/step - loss: -9.7586 - mean_squared_error: 10.8950 - val_loss: -18.9690 - val_mean_squared_error: 10.4393\n",
      "Epoch 45/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -9.7575 - mean_squared_error: 10.8926 - val_loss: -20.6049 - val_mean_squared_error: 10.7468\n",
      "Epoch 46/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -9.5017 - mean_squared_error: 11.1711 - val_loss: -21.0115 - val_mean_squared_error: 10.7209\n",
      "Epoch 47/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -9.8481 - mean_squared_error: 10.8906 - val_loss: -21.8996 - val_mean_squared_error: 10.9288\n",
      "Epoch 48/50\n",
      "94/94 [==============================] - 0s 5ms/step - loss: -10.1148 - mean_squared_error: 11.1226 - val_loss: -23.2955 - val_mean_squared_error: 10.9111\n",
      "Epoch 49/50\n",
      "94/94 [==============================] - 0s 4ms/step - loss: -9.9804 - mean_squared_error: 11.1769 - val_loss: -24.0907 - val_mean_squared_error: 11.0026\n",
      "Epoch 50/50\n",
      "94/94 [==============================] - 0s 3ms/step - loss: -9.9743 - mean_squared_error: 11.2801 - val_loss: -25.5005 - val_mean_squared_error: 10.9180\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Construct model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Assuming the target columns in y_train, y_val, y_test are single columns\n",
    "#y_train = y_train.values.ravel()\n",
    "#y_val = y_val.values.ravel()\n",
    "#y_test = y_test.values.ravel()\n",
    "\n",
    "\n",
    "#Add custom loss function\n",
    "def sharpe_loss(y_true, y_pred):\n",
    "    # Assuming y_pred are returns predictions and y_true are actual returns\n",
    "    # Calculate expected return and volatility (standard deviation)\n",
    "    returns = tf.reduce_mean(y_pred)  # Expected return\n",
    "    volatility = tf.math.reduce_std(y_pred)  # Standard deviation of returns\n",
    "    sharpe_ratio = returns / (volatility + 1e-6)  # Added a small constant to avoid division by zero\n",
    "    return -sharpe_ratio  # Minimize the negative Sharpe ratio (i.e., maximize the Sharpe ratio)\n",
    "\n",
    "\n",
    "\n",
    "#Construct model\n",
    "\n",
    "input_shape=11\n",
    "\n",
    "def build_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', kernel_initializer=HeNormal(), input_shape=(input_shape,)),\n",
    "        Dropout(0.1),\n",
    "        Dense(64, activation='relu', kernel_initializer=HeNormal()),\n",
    "        Dropout(0.1),\n",
    "        Dense(1, activation='linear')  # Assuming a binary classification problem\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=.0001), loss=sharpe_loss, metrics=['mean_squared_error'])\n",
    "    return model\n",
    "\n",
    "model = build_model(input_shape)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val))\n",
    "\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='validation loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cd85cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
